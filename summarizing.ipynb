{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reorder(output, input ):\n",
    "    output.sort( lambda s1, s2: input.find(s1) - input.find(s2) )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summarized(input, num_sentences ):\n",
    "    \n",
    "    # input : The original passage; here, the original transcript\n",
    "    # num_sentences : The number of sentences you want in your summary as output.\n",
    "    \n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "    # Calculating the frequency of each word in the input\n",
    "    base_words = [word.lower() for word in tokenizer.tokenize(input)]\n",
    "    words = [word for word in base_words if word not in stopwords.words()]\n",
    "    word_frequencies = FreqDist(words)\n",
    "\n",
    "    # Extracting the most frequent words\n",
    "    most_frequent_words = [pair[0] for pair in word_frequencies.items()[:100]]\n",
    "    \n",
    "    # Break the input into sentences. \"working_sentences\" is used for the analysis,\n",
    "    # but \"actual_sentences\" is used in the summary\n",
    "    \n",
    "    sentences = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    actual_sentences = sentences.tokenize(input)\n",
    "    working_sentences = [s.lower() for s in actual_sentences]\n",
    "\n",
    "    # Iterate over the most frequent words, and add the first sentence that inclues each word to the result.\n",
    "    output_sentences = []\n",
    "\n",
    "    for word in most_frequent_words:\n",
    "        for i in range(0, len(working_sentences)):\n",
    "            if (word in working_sentences[i]\n",
    "              and actual_sentences[i] not in output_sentences):\n",
    "                output_sentences.append(actual_sentences[i])\n",
    "                break\n",
    "            if len(output_sentences) >= num_sentences: break\n",
    "        if len(output_sentences) >= num_sentences: break\n",
    "\n",
    "    # Sort the output sentences back to their original order\n",
    "    return reorder(output_sentences, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(input, num_sentences):\n",
    "    print \" \".join(get_summarized(input, num_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Around that same time, Stanley Kubrick's \"2001: A Space Odyssey\" came to the theaters, and my life was forever changed. Now, HAL was a fictional character, but nonetheless he speaks to our fears, our fears of being subjugated by some unfeeling, artificial intelligence who is indifferent to our humanity. Indeed, we stand at a remarkable time in human history, where, driven by refusal to accept the limits of our bodies and our minds, we are building machines of exquisite, beautiful complexity and grace that will extend the human experience in ways beyond our imagining. Another fascinating idea in the mission profile places humanoid robots on the surface of Mars before the humans themselves arrive, first to build facilities and later to serve as collaborative members of the science team. Can we build a system that sets goals, that carries out plans against those goals and learns along the way? Consider this: building a cognitive system is fundamentally different than building a traditional software-intensive system of the past. To teach a system how to play a game like Go, I'd have it play thousands of games of Go, but in the process I also teach it how to discern a good game from a bad game. If I want to create an artificially intelligent legal assistant, I will teach it some corpus of law but at the same time I am fusing with it the sense of mercy and justice that is part of that law. Indeed, in the book \"Superintelligence\" by the philosopher Nick Bostrom, he picks up on this theme and observes that a superintelligence might not only be dangerous, it could represent an existential threat to all of humanity. To worry now about the rise of a superintelligence is in many ways a dangerous distraction because the rise of computing itself brings to us a number of human and societal issues to which we must now attend.\n"
     ]
    }
   ],
   "source": [
    "f = open('text_mod.txt','r+')\n",
    "f1 = ''.join([i if ord(i) < 128 else ' ' for i in f.read()]) \n",
    "summarize(f1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
